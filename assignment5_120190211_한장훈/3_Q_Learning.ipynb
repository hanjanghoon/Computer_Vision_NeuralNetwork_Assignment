{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"3_Q_Learning.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UQfz8HDWxHO2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592381183284,"user_tz":-540,"elapsed":1405,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLarxtmJxHO8","colab_type":"text"},"source":["## Q learning (Off-Policy TD Control)"]},{"cell_type":"markdown","metadata":{"id":"jCi1MN7vxHO9","colab_type":"text"},"source":["same in \"2_MC Control agent.ipynb\""]},{"cell_type":"code","metadata":{"id":"IP87RaHzxHO-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592381183756,"user_tz":-540,"elapsed":1865,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}}},"source":["class Env:\n","    def __init__(self):\n","        self.grid_width = 5\n","        self.grid_height = self.grid_width\n","        self.action_grid = [(-1, 0), (1, 0), (0, -1), (0, 1)]     # U, D, L, R\n","        self.gtriangle1 = [1, 2]\n","        self.gtriangle2 = [2, 1]\n","        self.goal = [2, 2]\n","        \n","    def step(self, state, action):\n","        x, y = state\n","        \n","        # get next state by action\n","        x+= action[0]\n","        y+= action[1]\n","        \n","        if x < 0 :\n","            x = 0\n","        elif x > (self.grid_width-1) :\n","            x = (self.grid_width-1)\n","\n","        if y < 0 :\n","            y = 0\n","        elif y > (self.grid_width-1) :\n","            y = (self.grid_width-1)\n","        \n","        next_state = [x, y]\n","        \n","        # reward \n","        if next_state == self.gtriangle1 or next_state == self.gtriangle2:\n","            reward = -1\n","            done = True\n","        elif next_state == self.goal:\n","            reward = 1\n","            done = True\n","        else:\n","            reward = 0\n","            done = False\n","        \n","        return next_state, reward, done\n","    \n","    def reset(self):\n","        return [0, 0]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJ_CKs_PxHPB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592381183757,"user_tz":-540,"elapsed":1861,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}}},"source":["class Qlearning_agent:\n","    def __init__(self):\n","        self.action_grid = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n","        self.action_text= ['U', 'D', 'L', 'R']\n","        self.grid_width = 5\n","        self.grid_height = self.grid_width\n","        self.Qtable = np.zeros((self.grid_width, self.grid_height, len(self.action_grid)))\n","        self.e = .1\n","        self.learning_rate = .01\n","        self.discount_factor = .95\n","        self.memory=[]\n","    \n","    def get_action(self, state):\n","        # with prob.ε take random action\n","        if np.random.randn() <  self.e :\n","            idx = np.random.choice(len(self.action_grid),1)[0]\n","        else :\n","            Qvalues = self.Qtable[tuple(state)]\n","            maxQ = np.amax(Qvalues)\n","            tie_Qchecker = np.where(Qvalues==maxQ)[0]\n","            \n","            # if tie max value, get random\n","            if len(tie_Qchecker) > 1:\n","                idx = np.random.choice(tie_Qchecker, 1)[0]\n","            else :\n","                idx = np.argmax(Qvalues)\n","                \n","        action = self.action_grid[idx]\n","        return action    \n","        \n","    # using First visit MC    \n","    def update(self, state, action, reward, next_state):\n","        action_idx = self.action_grid.index(action)\n","        '''수정한 부분 '''\n","        #next_action_idx = self.action_grid.index(next_action)\n","        current_Q = self.Qtable[tuple(state)][action_idx]\n","        next_Q = max(self.Qtable[tuple(next_state)])\n","\n","        #print(next_Q)\n","        #print(\"끝\")\n","        updated_Q = current_Q + self.learning_rate*((reward + self.discount_factor*next_Q)-current_Q)\n","        \n","        self.Qtable[tuple(state)][action_idx] = updated_Q\n","        \n","    def save_actionseq(self, action_sequence, action):\n","        idx = self.action_grid.index(action)\n","        action_sequence.append(self.action_text[idx])"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATus6M3UxHPD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592381187636,"user_tz":-540,"elapsed":5724,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}},"outputId":"402f0593-2e83-4d69-d7d2-3dc147149355"},"source":["if __name__ =='__main__':\n","    env = Env()\n","    agent = Qlearning_agent()\n","    total_episode = 10000\n","    sr = 0\n","    \n","    for episode in range(total_episode):\n","        action_sequence=[]\n","        total_reward = 0\n","        walk = 0\n","        \n","        # initial state, action, done\n","        state = env.reset()\n","        action = agent.get_action(state)\n","        done = False\n","        \n","        while not done:  \n","            agent.save_actionseq(action_sequence, action)\n","            '''수정한 부분 '''\n","            # next state, action\n","            next_state, reward, done = env.step(state, action)\n","            #next_action = agent.get_action(next_state)\n","\n","            # update Qtable\n","            agent.update(state, action, reward, next_state)\n","            \n","            total_reward += reward\n","            state = next_state\n","            action = agent.get_action(state)\n","            \n","            if done:\n","                if episode % 100 == 0:\n","                    print('finished at', next_state)\n","                    print('episode :{}, The number of step:{}\\n The sequence of action is:\\\n","                          {}\\nThe total reward is: {}\\n'.format(episode, walk, action_sequence, total_reward))\n","                if state == env.goal:\n","                    sr += 1\n","                break\n","\n","            \n","            \n","            \n","print('The accuracy :', sr/total_episode*100, '%')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["finished at [2, 1]\n","episode :0, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'L', 'R', 'U', 'R', 'R', 'L', 'L', 'D', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :100, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'U', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :200, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'R', 'R', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :300, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'D', 'U', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :400, The number of step:0\n"," The sequence of action is:                          ['D', 'R', 'U', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :500, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'U', 'L', 'R', 'D', 'U', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :600, The number of step:0\n"," The sequence of action is:                          ['D', 'D', 'L', 'U', 'U', 'R', 'R', 'L', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :700, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'R', 'U', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :800, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'R', 'U', 'L', 'L']\n","The total reward is: -1\n","\n","finished at [2, 1]\n","episode :900, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'L', 'D', 'L', 'D', 'R']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :1000, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'R', 'D', 'R', 'L', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :1100, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'L', 'U', 'R', 'L', 'R', 'R', 'L', 'R', 'R', 'D', 'U', 'D', 'D', 'R', 'L', 'D', 'U', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :1200, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'L', 'R', 'R', 'R', 'L', 'R', 'D', 'R', 'L', 'D', 'U', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :1300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'R', 'R', 'L', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :1400, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'L', 'U', 'R', 'R', 'U', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :1500, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'R', 'L', 'L', 'R', 'R', 'U', 'R', 'U', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :1600, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'L', 'R', 'L', 'R', 'U', 'L', 'R', 'D', 'D', 'U', 'U', 'D', 'D', 'D', 'U', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :1700, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :1800, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'L', 'R', 'U', 'R', 'R', 'D', 'D', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :1900, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'R', 'L', 'R', 'R', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :2000, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :2100, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'R', 'L', 'U', 'R', 'U', 'U', 'R', 'R', 'L', 'R', 'L', 'D', 'D', 'R', 'D', 'D', 'U', 'U', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :2200, The number of step:0\n"," The sequence of action is:                          ['L', 'R', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :2300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :2400, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :2500, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :2600, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'R', 'R', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 1]\n","episode :2700, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :2800, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'U', 'R', 'R', 'L', 'R', 'U', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :2900, The number of step:0\n"," The sequence of action is:                          ['L', 'R', 'R', 'R', 'D', 'D', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :3000, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :3100, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'L', 'D', 'L', 'U', 'R', 'R', 'R', 'U', 'D', 'D', 'D', 'D', 'U', 'U', 'D', 'D', 'U', 'R', 'L', 'U', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :3200, The number of step:0\n"," The sequence of action is:                          ['U', 'D', 'D', 'U', 'R', 'U', 'U', 'U', 'D', 'U', 'R', 'R', 'L', 'R', 'D', 'D', 'R', 'L', 'D', 'U', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :3300, The number of step:0\n"," The sequence of action is:                          ['L', 'R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :3400, The number of step:0\n"," The sequence of action is:                          ['L', 'U', 'U', 'R', 'D', 'U', 'R', 'R', 'R', 'L', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :3500, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'R']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :3600, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'U', 'R', 'R', 'D', 'U', 'L', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :3700, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :3800, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :3900, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'L', 'L', 'D', 'R']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :4000, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'L', 'U', 'R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :4100, The number of step:0\n"," The sequence of action is:                          ['D', 'L', 'U', 'R', 'R', 'R', 'U', 'D', 'D', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :4200, The number of step:0\n"," The sequence of action is:                          ['L', 'R', 'R', 'L', 'D', 'U', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :4300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'L', 'D', 'L', 'U', 'R', 'R', 'L', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :4400, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'R', 'R', 'L', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :4500, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'R', 'U', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :4600, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'R', 'R', 'D', 'D', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :4700, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'U', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :4800, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :4900, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'U', 'U', 'D', 'D', 'D', 'U', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :5000, The number of step:0\n"," The sequence of action is:                          ['L', 'R', 'L', 'R', 'U', 'U', 'R', 'R', 'R', 'U', 'L', 'D', 'R', 'L', 'U', 'D', 'R', 'L', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :5100, The number of step:0\n"," The sequence of action is:                          ['D', 'D', 'U', 'U', 'R', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :5200, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'R', 'L', 'U', 'R', 'R', 'R', 'R', 'L', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :5300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'L', 'R', 'U', 'D', 'D', 'U', 'D', 'R', 'U', 'U', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :5400, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'R', 'R', 'L', 'U', 'R', 'U', 'U', 'L', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :5500, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :5600, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :5700, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'R', 'D', 'D', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :5800, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'D', 'U', 'R', 'D', 'L', 'U', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :5900, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'R', 'L', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 1]\n","episode :6000, The number of step:0\n"," The sequence of action is:                          ['D', 'D', 'R']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :6100, The number of step:0\n"," The sequence of action is:                          ['L', 'R', 'D', 'U', 'R', 'R', 'U', 'D', 'U', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :6200, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :6300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'L', 'R', 'R', 'D', 'D', 'U', 'R', 'D', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :6400, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'L', 'L', 'R', 'U', 'R', 'D', 'R', 'D', 'R', 'R', 'U', 'L', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :6500, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'R', 'U', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :6600, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'R', 'U', 'R', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :6700, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'R', 'R', 'R', 'U', 'U', 'U', 'L', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :6800, The number of step:0\n"," The sequence of action is:                          ['L', 'D', 'U', 'R', 'L', 'R', 'D', 'U', 'R', 'R', 'D', 'D', 'U', 'U', 'R', 'L', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :6900, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'L', 'D', 'U', 'U', 'R', 'R', 'R', 'D', 'R', 'L', 'L']\n","The total reward is: -1\n","\n","finished at [2, 1]\n","episode :7000, The number of step:0\n"," The sequence of action is:                          ['U', 'L', 'D', 'U', 'D', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :7100, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'R', 'L', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :7200, The number of step:0\n"," The sequence of action is:                          ['R', 'L', 'U', 'R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :7300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :7400, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'L', 'D', 'R', 'R']\n","The total reward is: -1\n","\n","finished at [2, 1]\n","episode :7500, The number of step:0\n"," The sequence of action is:                          ['R', 'L', 'D', 'D', 'R']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :7600, The number of step:0\n"," The sequence of action is:                          ['R', 'L', 'L', 'L', 'R', 'R', 'R', 'U', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :7700, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'R', 'R', 'D', 'U', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :7800, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'L', 'R', 'R', 'L', 'R', 'R', 'D', 'D', 'D', 'U', 'D', 'D', 'D', 'R', 'L', 'U', 'U', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :7900, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'L', 'R', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :8000, The number of step:0\n"," The sequence of action is:                          ['D', 'U', 'L', 'L', 'U', 'R', 'L', 'L', 'R', 'R', 'L', 'D', 'L', 'U', 'R', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :8100, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :8200, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :8300, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'D']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :8400, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'D', 'L', 'L', 'U', 'R', 'L', 'L', 'R', 'D', 'R']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :8500, The number of step:0\n"," The sequence of action is:                          ['R', 'L', 'R', 'R', 'U', 'R', 'R', 'R', 'L', 'L', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :8600, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'U', 'U', 'U', 'L', 'R', 'R', 'L', 'D', 'D', 'U', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :8700, The number of step:0\n"," The sequence of action is:                          ['L', 'D', 'U', 'U', 'R', 'R', 'R', 'D', 'D', 'U', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :8800, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'L', 'L', 'R', 'U', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :8900, The number of step:0\n"," The sequence of action is:                          ['R', 'L', 'L', 'R', 'R', 'R', 'D', 'D', 'R', 'L', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :9000, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'R', 'R', 'L', 'U', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :9100, The number of step:0\n"," The sequence of action is:                          ['U', 'R', 'L', 'D', 'U', 'D', 'L', 'U', 'D', 'L', 'U', 'D', 'U', 'R', 'R', 'L', 'D', 'L', 'U', 'R', 'R', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :9200, The number of step:0\n"," The sequence of action is:                          ['L', 'D', 'U', 'U', 'D', 'D', 'D', 'U', 'U', 'R', 'U', 'L', 'R', 'R', 'L', 'R', 'U', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [1, 2]\n","episode :9300, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'U', 'U', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :9400, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'L', 'R', 'R', 'D', 'D', 'R', 'L', 'U', 'U', 'U', 'D', 'R', 'L', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [2, 2]\n","episode :9500, The number of step:0\n"," The sequence of action is:                          ['U', 'D', 'U', 'R', 'R', 'R', 'D', 'U', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :9600, The number of step:0\n"," The sequence of action is:                          ['R', 'D', 'U', 'R', 'R', 'D', 'L']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :9700, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'R', 'D', 'D', 'L']\n","The total reward is: 1\n","\n","finished at [1, 2]\n","episode :9800, The number of step:0\n"," The sequence of action is:                          ['R', 'R', 'U', 'D']\n","The total reward is: -1\n","\n","finished at [2, 2]\n","episode :9900, The number of step:0\n"," The sequence of action is:                          ['R', 'U', 'L', 'R', 'R', 'R', 'D', 'D', 'R', 'R', 'D', 'L', 'R', 'L', 'U', 'L']\n","The total reward is: 1\n","\n","The accuracy : 55.059999999999995 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n14KqqyQxHPH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":989},"executionInfo":{"status":"ok","timestamp":1592381187639,"user_tz":-540,"elapsed":5718,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}},"outputId":"5db2363a-a823-4c60-cd02-1c5c8103250d"},"source":["agent.Qtable"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 7.35091891e-01,  6.98337295e-01,  7.35091891e-01,\n","          7.73780937e-01],\n","        [ 7.73780937e-01,  7.35091815e-01,  7.35091890e-01,\n","          8.14506250e-01],\n","        [ 8.14506239e-01, -9.99999999e-01,  7.73780932e-01,\n","          8.57375000e-01],\n","        [ 8.57374983e-01,  9.02500000e-01,  8.14506231e-01,\n","          8.14505820e-01],\n","        [ 7.97292677e-01,  8.33133656e-01,  8.57374967e-01,\n","          7.89569452e-01]],\n","\n","       [[ 7.35091891e-01,  6.17797229e-01,  6.92739288e-01,\n","          7.29447815e-01],\n","        [ 7.73780916e-01, -9.86177299e-01,  6.75959714e-01,\n","         -9.87624007e-01],\n","        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","          0.00000000e+00],\n","        [ 8.57374459e-01,  9.50000000e-01, -9.99999632e-01,\n","          8.57368592e-01],\n","        [ 7.77330774e-01,  8.50240166e-01,  9.02499489e-01,\n","          8.10270074e-01]],\n","\n","       [[ 6.85146123e-01,  7.16630965e-02,  2.76190716e-01,\n","         -6.26535720e-01],\n","        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","          0.00000000e+00],\n","        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n","          0.00000000e+00],\n","        [ 9.02491823e-01,  9.02365988e-01,  1.00000000e+00,\n","          9.02332926e-01],\n","        [ 7.48895471e-01,  6.57084738e-01,  9.49979262e-01,\n","          7.98547968e-01]],\n","\n","       [[ 2.71555487e-01,  7.54641202e-04,  1.08420063e-02,\n","          3.88181161e-02],\n","        [-2.45280713e-01,  7.32752424e-04,  1.01054048e-02,\n","          2.66244713e-01],\n","        [ 9.07627836e-01,  6.29562166e-02,  4.75476225e-02,\n","          4.06550145e-01],\n","        [ 9.49985908e-01,  6.16745217e-01,  6.87549812e-01,\n","          6.56329360e-01],\n","        [ 4.62432437e-01,  1.33982553e-01,  8.70878224e-01,\n","          3.25569480e-01]],\n","\n","       [[ 1.46835661e-02,  6.54635613e-05,  2.26186302e-04,\n","          7.02817387e-04],\n","        [ 2.82750586e-02,  1.87233954e-04,  1.17453826e-04,\n","          3.85037047e-03],\n","        [ 3.48776675e-01,  1.74061619e-02,  1.22283622e-03,\n","          8.73900853e-02],\n","        [ 8.38814541e-01,  2.28135989e-01,  6.95450862e-02,\n","          1.10452800e-01],\n","        [ 1.53459784e-01,  2.35864888e-02,  4.51475868e-01,\n","          3.24753896e-02]]])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"99gMXyszxHPK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592381187641,"user_tz":-540,"elapsed":5714,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}},"outputId":"217dadfc-a724-4892-fe8c-545a4c745e22"},"source":["agent.Qtable[0,1]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.77378094, 0.73509181, 0.73509189, 0.81450625])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"QUMA_HALxHPN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592381187642,"user_tz":-540,"elapsed":5709,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}},"outputId":"eaf89628-0a6c-4f21-cb54-04b79b15d44b"},"source":["agent.Qtable[1,1]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.77378092, -0.9861773 ,  0.67595971, -0.98762401])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Fa69qyl3xHPS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592381187644,"user_tz":-540,"elapsed":5705,"user":{"displayName":"jh a","photoUrl":"","userId":"12317339413270191329"}},"outputId":"25fd11b9-add0-4729-e79b-4d87b2bad0c1"},"source":["agent.Qtable[1,0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.73509189, 0.61779723, 0.69273929, 0.72944782])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"YsfV9BX1AvDf","colab_type":"text"},"source":["\n","1. 설명\n","\n","\n","> 현재 action을 취하고 있는 policy와 improve 하는 대상의 policy가 같을 경우 on policy라고 한다. 이는 greedy한 방법을 사용하는데 한계가 존재한다. e-greedy 방법을 사용하는 대신 action을 취하는 policy 와 업데이트 하는 polict를 다르게 취급하여 학습할 수 있는데 이를 off-policy 방식이라고 한다. Q-learning은 off-policy TD learning의 대표적인 방법으로써 2의 SARSA를 Q-learning으로 수정하였다. off-policy 방식은 여러가지 장점이 있는데 그 장점중 제일 가장 큰 장점은 exploration을 하면서도 optimal한 policy를 학습할수 있다는 것이다. 이렇게 가능한 이유는 action을 취하는 policy(behavior policy)와 improve하는 policy(target policy)를 다르게 하기 때문인데 behavior policy는 e-greedy , target policy greedy 방식으로 학습하게 된다.\n","\n","\n","> 코드를 보면 get_action에서 behavior policy 에서 action을 선택하는건 같지만 update 함수가 달라진것을 확인 할 수 있다. SARSA의 식에서 nextQ로 업데이트할때 next_Q를 get_action함수를 통해 next action을 받아와서 업데이트 하는 반면에 Q-learning에서는 optimalQ (max값을 가지는 Q)를 greedy 방식으로 선택 하게 하여 업데이트한다. 즉 next action을 따로 get_action으로 받아오는것이 아닌 다음상태에서 최대가되는 maxQ를 사용하는 것이다. 따라서 코드를 보면 next_action을 따로 만들지도 않고 업데이트에서 사용하지도 않는다.\n","\n","2. 성능 분석\n","\n","\n","> 성능의 경우 55%정도로 SARSA의 방법과 크게 차이가 나지 않는다. grid world 특성상 2가지 알고리즘 모두 적용 가능하기 때문이다. 하지만 Cliff walking 같은 예제는 다른결과를 낸다. SARSA의 경우는 exploration을 하다가 cliff 에 빠지면 주변 state 모두 낮은 value function을 가져 결국 cliff 쪽으로 가지 않게 된다. 하지만 Q-learning의 경우 cliff에 빠졌더고 해도 거기에 해당하는 action-value function만 낮아지므로 그 근처의 경로를 사용가능하다. 단지 cliff 쪽으로 가지 않는 action만 학습하게 된다. 따라서 SARSA와 달리 cliff와 가까운 optimal path를 찾을 수 있다.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PPkbTx9LH9yX","colab_type":"text"},"source":["![](https://drive.google.com/file/d/1bAS4PvVj5R1brKIFlueco6mmFRWJqXAZ/view?usp=sharing)\n"]}]}